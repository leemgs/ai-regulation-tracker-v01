from __future__ import annotations
import re
from typing import List, Set, Tuple
from .utils import debug_log

def extract_section(md_text: str, section_title: str) -> str:
    """Markdown í…ìŠ¤íŠ¸ì—ì„œ íŠ¹ì • ì„¹ì…˜ ì œëª© ì•„ë˜ì˜ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    lines = md_text.split("\n")
    start = None
    end = None
    for i, line in enumerate(lines):
        if line.strip().startswith(section_title):
            start = i + 1
            continue
        if start and line.startswith("## "):
            end = i
            break
    if start is None:
        return ""
    if end is None:
        end = len(lines)
    return "\n".join(lines[start:end])

def parse_table(section_md: str) -> Tuple[List[str], List[List[str]], Tuple[str, str]]:
    """Markdown í…Œì´ë¸”ì„ í—¤ë”, í–‰ ë°ì´í„°, ë©”íƒ€ë°ì´í„°(í—¤ë”/êµ¬ë¶„ì„  ë¼ì¸)ë¡œ íŒŒì‹±í•©ë‹ˆë‹¤."""
    lines = [l for l in section_md.split("\n") if l.strip().startswith("|")]
    if len(lines) < 3:
        return [], [], ("", "")

    header = lines[0]
    separator = lines[1]
    rows = lines[2:]

    def split_row(row_text: str) -> List[str]:
        # ì—­ìŠ¬ë˜ì‹œë¡œ ì´ìŠ¤ì¼€ì´í”„ë˜ì§€ ì•Šì€ íŒŒì´í”„ë§Œ ë¶„í• 
        return [c.strip() for c in re.split(r'(?<!\\)\|', row_text.strip())[1:-1]]

    header_cols = split_row(header)
    parsed_rows = []
    for row in rows:
        cols = split_row(row)
        if len(cols) == len(header_cols):
            parsed_rows.append(cols)
        else:
            debug_log(f"Table row column mismatch: expected {len(header_cols)}, got {len(cols)}. Row: {row[:100]}...")

    return header_cols, parsed_rows, (header, separator)

def extract_article_url(cell: str) -> str | None:
    """Markdown ë§í¬ ì…€ì—ì„œ URLì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
    m = re.search(r"\((https?://[^\)]+)\)", cell)
    if m:
        return m.group(1).split("&hl=")[0]
    return None

def apply_deduplication(md: str, comments: List[dict]) -> Tuple[str, dict | None]:
    """
    ì´ì „ GitHub ëŒ“ê¸€ë“¤ì„ ë¶„ì„í•˜ì—¬ ì¤‘ë³µëœ ë°ì´í„°ë¥¼ 'skip' ì²˜ë¦¬í•˜ê³  ìš”ì•½ì„ ì¶”ê°€í•©ë‹ˆë‹¤.
    """
    if not comments:
        return md, None

    # 1) Base Snapshot Key Set ìƒì„± (ëª¨ë“  ì´ì „ ëŒ“ê¸€ ëŒ€ìƒ)
    base_article_set: Set[str] = set()
    base_docket_set: Set[str] = set()

    for comment in comments:
        body = comment.get("body") or ""
        
        # News ì²˜ë¦¬
        news_section_base = extract_section(body, "## ğŸ“° AI Regulation News")
        h_news, r_news, _ = parse_table(news_section_base)
        if "ì œëª©" in h_news:
            idx = h_news.index("ì œëª©")
            for r in r_news:
                url = extract_article_url(r[idx])
                if url:
                    base_article_set.add(url)

    # 2) í˜„ì¬ Markdown ì²˜ë¦¬ (News)
    current_md = md
    news_section = extract_section(current_md, "## ğŸ“° AI Regulation News")
    n_headers, n_rows, n_table_meta = parse_table(news_section)

    new_article_count = 0
    total_article_count = len(n_rows)

    if n_headers and "ì œëª©" in n_headers:
        title_idx = n_headers.index("ì œëª©")
        no_idx = n_headers.index("No.") if "No." in n_headers else None
        date_idx = n_headers.index("ê¸°ì‚¬ì¼ìâ¬‡ï¸") if "ê¸°ì‚¬ì¼ìâ¬‡ï¸" in n_headers else None

        header_line, separator_line = n_table_meta
        non_skip_rows = []

        for r in n_rows:
            url = extract_article_url(r[title_idx])
            if url in base_article_set:
                debug_log(f"Skipping duplicate News: {r[title_idx]} ({url})")
            else:
                non_skip_rows.append(r)
                new_article_count += 1
        
        if new_article_count == 0:
            new_news_section = "ìƒˆë¡œìš´ ê·œì œ ì†Œì‹ì´ 0ê±´ì…ë‹ˆë‹¤.\n"
        else:
            final_rows = non_skip_rows
            new_lines = [header_line, separator_line]
            for row_idx, r in enumerate(final_rows, start=1):
                if no_idx is not None:
                    r[no_idx] = str(row_idx)
                new_lines.append("| " + " | ".join(r) + " |")
            new_news_section = "\n".join(new_lines)
        current_md = current_md.replace(news_section, new_news_section)


    # 4) ì¤‘ë³µ ì œê±° ìš”ì•½ ìƒì„±
    base_news_count = len(base_article_set)
    dup_news_count = total_article_count - new_article_count

    new_label = f"{new_article_count} (New)"
    if new_article_count > 0:
        new_label = f"ğŸ”´ **{new_label}**"

    summary_header = (
        "### ì¤‘ë³µ ì œê±° ìš”ì•½:\n"
        "ğŸ” Dedup Summary\n"
        f"â”” News {base_news_count} (Baseline): "
        f"{dup_news_count} (Dup), "
        f"{new_label}\n\n"
    )

    stats = {
        "base_news": base_news_count,
        "dup_news": dup_news_count,
        "new_news": new_article_count,
    }

    return summary_header + current_md, stats
